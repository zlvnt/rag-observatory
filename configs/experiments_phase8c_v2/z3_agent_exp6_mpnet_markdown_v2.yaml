# Phase 8C v2: MarkdownHeaderTextSplitter + MPNet + Increased k
# Goal: Test if higher k compensates for smaller chunks from Markdown splitter
#
# Baseline: exp6_mpnet_markdown (MPNet + Markdown + k=3)
#   - Precision: 0.711, Recall: 0.917, F1: 0.745
#   - Tokens/Query: 126 (very small chunks!)
#
# Hypothesis: Markdown creates smaller chunks (~50 tokens vs ~105 tokens)
#             k=3 insufficient for chunk coverage with MPNet embedding
#
# Changes from exp6_mpnet_markdown:
#   - retrieval_k: 3 → 5 (increase chunk coverage)
#   - KEEP: threshold=0.3, MPNet embedding, Markdown splitter
#
# Expected Results:
# - Precision: 0.711 → 0.77-0.80 (better coverage = more relevant)
# - Recall: 0.917 → 0.93-0.95 (retrieve more chunks)
# - F1: 0.745 → 0.82-0.85
# - Tokens: 126 → ~210 (5 chunks × 42 tokens)
# - May REACH target 0.80 precision! ✅

domain_name: z3_agent_exp6_mpnet_markdown_v2

knowledge_base_dir: docs/
vector_store_dir: data/vector_stores/z3_agent_exp6_mpnet_markdown/  # Reuse same index
golden_dataset: golden_datasets/z3_agent_tests.json

personality_config_path: content/reply_config1.json
supervisor_prompt_path: content/supervisor-prompt.txt

# Embedding: MPNet (proven best from Phase 8B)
embedding_model: sentence-transformers/paraphrase-multilingual-mpnet-base-v2

llm_model: gemini-2.0-flash
llm_temperature: 0.7

# RAG parameters - TUNED for Markdown splitter
relevance_threshold: 0.3  # Keep same
chunk_size: 500
chunk_overlap: 50
retrieval_k: 5  # ← INCREASED from 3 to 5

# Text Splitter: MarkdownHeaderTextSplitter (via test_runner_markdown.py)
