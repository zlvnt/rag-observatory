# Phase 9 Roadmap - Advanced Retrieval Techniques

**Date:** 2025-10-25
**Status:** Ready to start | Phase 8 complete (basic optimization exhausted)

---

## ğŸ¯ Phase 9 Overview

**Goal:** Bridge the 2.2% precision gap (0.783 â†’ 0.80+) using advanced retrieval techniques

**Current State:**
- Winner: Exp6 (MPNet + RecursiveCharacterTextSplitter + k=3)
- Precision: 0.783 (2.2% gap to 0.80 target)
- Recall: 0.917 (exceeds target)
- F1: 0.795 (exceeds target)

**Phase 8 Findings (What We Exhausted):**
- âœ… Embedding optimization complete (MPNet optimal, BGE-M3 failed)
- âœ… Splitter optimization complete (Recursive optimal, Markdown failed)
- âœ… k parameter optimized (k=3 best)
- âœ… Threshold optimized (0.3 best)

**Remaining Failure Patterns (from Phase 8A):**
1. ğŸ”´ **Ranking issues (10%)** - Correct doc retrieved but ranked low
2. ğŸŸ  **"Meleset sedikit" (30%)** - Right doc, wrong subsection ranked higher
3. ğŸŸ¡ **Multi-doc failures (20%)** - Only 1 doc retrieved when 2+ expected
4. ğŸŸ¢ **Context cutting (40%)** - Splitter cuts mid-section (cannot fix with current tools)

**Phase 9 Strategy:**
- Focus on techniques that don't require changing basic parameters
- Add post-processing layers (reranking, diversity)
- No more embedding/splitter experiments

---

## ğŸ—ºï¸ Phase 9 Roadmap

### **Phase 9A: MMR (Maximal Marginal Relevance)** â³ NEXT

**Goal:** Improve diversity and reduce redundancy in retrieved chunks

**Duration:** 2-3 hours

**What is MMR:**
- Algorithm: Balance relevance vs diversity
- Formula: `MMR = Î» Ã— similarity(query, doc) - (1-Î») Ã— max_similarity(doc, selected_docs)`
- Built-in: Langchain FAISS retriever (no extra model needed)
- No download required

**Target Problem:**
- ğŸŸ¡ **Multi-doc failures (20%)** - Retrieve chunks from different docs
- Avoid redundant chunks from same section

**How it works:**
```
Standard retrieval (k=5):
â†’ Chunk 1: policy_returns.md (section A)
â†’ Chunk 2: policy_returns.md (section A, redundant!)
â†’ Chunk 3: policy_returns.md (section B)
â†’ Chunk 4: policy_returns.md (section C)
â†’ Chunk 5: troubleshooting_guide.md

MMR retrieval (k=5, Î»=0.5):
â†’ Chunk 1: policy_returns.md (section A)
â†’ Chunk 2: troubleshooting_guide.md (diverse!)
â†’ Chunk 3: contact_escalation.md (diverse!)
â†’ Chunk 4: policy_returns.md (section B, different from chunk 1)
â†’ Chunk 5: product_faq.md (diverse!)
```

**Implementation:**
- Modify `z3_core/rag.py` to use `max_marginal_relevance_search()`
- Test different Î» values: 0.3, 0.5, 0.7
  - Î»=1.0: Pure relevance (standard retrieval)
  - Î»=0.0: Pure diversity (not useful)
  - Î»=0.5: Balanced (recommended start)

**Experiments:**
1. **Exp9a_mmr_03:** Î»=0.3 (high diversity)
2. **Exp9a_mmr_05:** Î»=0.5 (balanced)
3. **Exp9a_mmr_07:** Î»=0.7 (high relevance)

**Config (constant):**
```yaml
embedding_model: sentence-transformers/paraphrase-multilingual-mpnet-base-v2
chunk_size: 500
chunk_overlap: 50
retrieval_k: 5  # Higher k for MMR (need candidates to diversify)
relevance_threshold: 0.3
mmr_lambda: 0.5  # Vary this
```

**Expected Results:**
- Precision: 0.783 â†’ 0.80-0.82 (+2-4%)
- Recall: Maintain 0.92+ (more docs covered)
- Multi-doc queries: Improved recall (from 0.50 â†’ 0.70+)

**Success Criteria:**
- Multi-doc recall improvement â‰¥ +15%
- Overall precision â‰¥ 0.80 (reach target!)
- No significant precision drop on easy queries

---

#### Task 1: Implement MMR Retrieval

**Modify:** `z3_core/rag.py`

**Current code:**
```python
def get_context(retriever, query: str, k: int = 4) -> tuple:
    docs = retriever.invoke(query)
    # ...
```

**New code:**
```python
def get_context(retriever, query: str, k: int = 4, use_mmr: bool = False,
                mmr_lambda: float = 0.5) -> tuple:
    if use_mmr:
        docs = retriever.max_marginal_relevance_search(
            query,
            k=k,
            fetch_k=k*3,  # Fetch 3x candidates for diversity selection
            lambda_mult=mmr_lambda
        )
    else:
        docs = retriever.invoke(query)
    # ...
```

**Parameters:**
- `fetch_k`: Number of candidates to fetch before MMR filtering (recommend k*3)
- `lambda_mult`: Balance between relevance (1.0) and diversity (0.0)

---

#### Task 2: Create MMR Test Configs

**Create folder:** `configs/experiments_phase9a/`

**Files:**
1. `z3_agent_exp9a_mmr_03.yaml`
2. `z3_agent_exp9a_mmr_05.yaml`
3. `z3_agent_exp9a_mmr_07.yaml`

**Example config:**
```yaml
domain_name: z3_agent_exp9a_mmr_05
knowledge_base_dir: docs/
vector_store_dir: data/vector_stores/z3_agent_exp6/  # Reuse Exp6 index

embedding_model: sentence-transformers/paraphrase-multilingual-mpnet-base-v2
chunk_size: 500
chunk_overlap: 50
retrieval_k: 5
relevance_threshold: 0.3

# MMR parameters
use_mmr: true
mmr_lambda: 0.5
mmr_fetch_k: 15  # 3x retrieval_k
```

---

#### Task 3: Run MMR Experiments

**Execute:**
```bash
python runners/test_runner.py --domain z3_agent_exp9a_mmr_03 --output results/exp9a_mmr_03/
python runners/test_runner.py --domain z3_agent_exp9a_mmr_05 --output results/exp9a_mmr_05/
python runners/test_runner.py --domain z3_agent_exp9a_mmr_07 --output results/exp9a_mmr_07/
```

**Compare with baseline:**
- Exp6 (k=3, no MMR): Precision 0.783
- Exp9a variants (k=5, MMR): Target precision 0.80+

---

#### Task 4: Analyze MMR Results

**Metrics to compare:**

| Config | Î» | k | Precision | Recall | F1 | Multi-doc Recall | Tokens/Query |
|--------|---|---|-----------|--------|----|--------------------|--------------|
| Exp6 (baseline) | - | 3 | 0.783 | 0.917 | 0.795 | 0.50 | 211 |
| Exp9a_mmr_03 | 0.3 | 5 | ??? | ??? | ??? | ??? | ??? |
| Exp9a_mmr_05 | 0.5 | 5 | ??? | ??? | ??? | ??? | ??? |
| Exp9a_mmr_07 | 0.7 | 5 | ??? | ??? | ??? | ??? | ??? |

**Key analyses:**
1. Does MMR improve multi-doc recall?
2. Does diversity hurt precision on easy queries?
3. What's the optimal Î» value?
4. Token efficiency impact (k=5 vs k=3)

---

### **Phase 9B: bge-reranker (Cross-Encoder Reranking)** â³ PLANNED

**Goal:** Improve ranking accuracy using neural cross-encoder model

**Duration:** 3-4 hours (includes model download)

**What is bge-reranker:**
- Model: BAAI/bge-reranker-v2-m3 (1.5GB) or BAAI/bge-reranker-base (600MB)
- Type: Cross-encoder (not bi-encoder like MPNet)
- Input: (query, document) pairs â†’ Output: relevance score (0-1)
- Much more accurate than bi-encoder embeddings

**Target Problem:**
- ğŸ”´ **Ranking issues (10%)** - Fix incorrect ranking
- ğŸŸ  **"Meleset sedikit" (30%)** - Prefer correct subsection over nearby text
- **Total 40% of failures!**

**How it works:**
```
Standard retrieval (k=7):
1. policy_returns.md (score: 0.85) â† WRONG section
2. troubleshooting_guide.md (score: 0.83) â† CORRECT but ranked low
3. product_faq.md (score: 0.80)
...

After reranking (top-3):
1. troubleshooting_guide.md (score: 0.95) â† CORRECT, now ranked #1!
2. policy_returns.md (score: 0.72) â† Dropped
3. contact_escalation.md (score: 0.68)
```

**Implementation:**
- Retrieve k=7 candidates (MPNet bi-encoder)
- Rerank with bge-reranker (cross-encoder)
- Return top-3 for LLM

**Why k=7 â†’ top-3:**
- Reranker needs more candidates to choose from
- k=3 too few for reranker to work effectively
- Cross-encoder slower, so filter down to top-3

---

#### Task 5: Install bge-reranker

**Install FlagEmbedding:**
```bash
pip install FlagEmbedding
```

**Download model (auto on first run):**
```python
from FlagEmbedding import FlagReranker

reranker = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True)
# Or smaller: 'BAAI/bge-reranker-base'
```

**Model size:**
- bge-reranker-v2-m3: ~1.5GB (best quality, multilingual)
- bge-reranker-base: ~600MB (good quality, faster)

**Recommendation:** Start with `bge-reranker-base` (smaller, faster)

---

#### Task 6: Implement Reranker Pipeline

**Create:** `z3_core/reranker.py`

```python
from FlagEmbedding import FlagReranker
from typing import List, Tuple
from langchain.schema import Document

class BGEReranker:
    def __init__(self, model_name: str = "BAAI/bge-reranker-base", use_fp16: bool = True):
        self.reranker = FlagReranker(model_name, use_fp16=use_fp16)

    def rerank(self, query: str, documents: List[Document], top_k: int = 3) -> List[Document]:
        """
        Rerank documents using cross-encoder model.

        Args:
            query: User query
            documents: List of retrieved documents
            top_k: Number of top documents to return

        Returns:
            Top-k reranked documents
        """
        # Prepare pairs for reranker
        pairs = [[query, doc.page_content] for doc in documents]

        # Get scores from reranker
        scores = self.reranker.compute_score(pairs)

        # Sort by score (descending)
        doc_score_pairs = list(zip(documents, scores))
        doc_score_pairs.sort(key=lambda x: x[1], reverse=True)

        # Return top-k
        return [doc for doc, score in doc_score_pairs[:top_k]]
```

**Modify:** `z3_core/rag.py`

```python
def get_context(retriever, query: str, k: int = 4, use_reranker: bool = False,
                reranker_top_k: int = 3) -> tuple:
    # Retrieve more candidates if using reranker
    retrieve_k = k * 2 if use_reranker else k
    docs = retriever.invoke(query, k=retrieve_k)

    # Rerank if enabled
    if use_reranker:
        from z3_core.reranker import BGEReranker
        reranker = BGEReranker()
        docs = reranker.rerank(query, docs, top_k=reranker_top_k)

    # ... rest of context processing
```

---

#### Task 7: Create Reranker Test Configs

**Create folder:** `configs/experiments_phase9b/`

**Files:**
1. `z3_agent_exp9b_reranker_base.yaml` (bge-reranker-base)
2. `z3_agent_exp9b_reranker_v2.yaml` (bge-reranker-v2-m3, if base succeeds)

**Example config:**
```yaml
domain_name: z3_agent_exp9b_reranker_base
knowledge_base_dir: docs/
vector_store_dir: data/vector_stores/z3_agent_exp6/  # Reuse Exp6 index

embedding_model: sentence-transformers/paraphrase-multilingual-mpnet-base-v2
chunk_size: 500
chunk_overlap: 50
retrieval_k: 7  # Retrieve more candidates
relevance_threshold: 0.3

# Reranker parameters
use_reranker: true
reranker_model: BAAI/bge-reranker-base
reranker_top_k: 3  # Final number to return
```

---

#### Task 8: Run Reranker Experiments

**Execute:**
```bash
python runners/test_runner.py --domain z3_agent_exp9b_reranker_base --output results/exp9b_reranker_base/
```

**Expected results:**
- Precision: 0.783 â†’ **0.83-0.85** (+5-7%)
- Recall: Maintain 0.92+
- Ranking: Correct doc should rank #1 more often
- MRR: 0.950 â†’ 0.97+ (better ranking)

---

#### Task 9: Compare Reranker vs Baseline

**Metrics comparison:**

| Config | Retrieval | Reranker | Precision | Recall | F1 | MRR | Tokens/Query |
|--------|-----------|----------|-----------|--------|----|----|--------------|
| Exp6 | MPNet k=3 | None | 0.783 | 0.917 | 0.795 | 0.950 | 211 |
| Exp9b_base | MPNet k=7 | bge-base | ??? | ??? | ??? | ??? | ??? |
| Exp9b_v2 | MPNet k=7 | bge-v2-m3 | ??? | ??? | ??? | ??? | ??? |

**Qualitative check:**
- Sample 5-10 failed queries from Exp6
- Check: Does reranker fix ranking issues?
- Example queries:
  - "Sudah bayar tapi status masih menunggu" (ranking issue in Exp6)
  - "OTP tidak masuk ke HP" (wrong doc ranked #1 in Exp6)

---

### **Phase 9C: Combined Approach (Optional)** â³ PLANNED

**Goal:** Test if MMR + Reranker can be combined

**Approach:**
```
Query â†’ Retrieve k=10 (MPNet) â†’ MMR (diverse 7) â†’ Rerank (top-3)
```

**Hypothesis:**
- MMR ensures diversity (different docs)
- Reranker ensures accuracy (correct ranking)
- Combined: Best of both worlds

**Only test if:**
- Both 9A and 9B show improvement
- Time permits

**Config:**
```yaml
retrieval_k: 10
use_mmr: true
mmr_lambda: 0.5
mmr_fetch_k: 30
mmr_return_k: 7
use_reranker: true
reranker_top_k: 3
```

---

## ğŸ“Š Deliverables Summary

### Phase 9A Outputs:
- â³ Modified `z3_core/rag.py` with MMR support
- â³ 3 MMR experiments (Î»=0.3, 0.5, 0.7)
- â³ MMR comparison table (metrics by Î»)
- â³ Best Î» value identified

### Phase 9B Outputs:
- â³ `z3_core/reranker.py` (BGEReranker class)
- â³ Modified `z3_core/rag.py` with reranker support
- â³ 1-2 reranker experiments (base/v2)
- â³ Reranker comparison table
- â³ Qualitative analysis of ranking improvements

### Phase 9C Outputs (Optional):
- â³ Combined MMR + Reranker experiment
- â³ Final comparison table (all Phase 9 variants)

### Final Documentation:
- â³ `PHASE_9_SUMMARY.md` - Complete Phase 9 analysis
- â³ Updated `PROGRESS.md`
- â³ Production config decision (Exp6 vs Exp9 winner)

---

## ğŸ¯ Success Criteria

**Phase 9 is successful if:**
1. âœ… At least ONE technique improves precision to â‰¥ 0.80 (reach target!)
2. âœ… Recall maintained â‰¥ 0.90
3. âœ… F1 score â‰¥ 0.82
4. âœ… Identified production-ready configuration

**Stretch goals:**
- ğŸ¯ Precision â‰¥ 0.85
- ğŸ¯ Multi-doc recall â‰¥ 0.70 (from 0.50)
- ğŸ¯ MRR â‰¥ 0.97
- ğŸ¯ All categories â‰¥ 0.75 precision

---

## ğŸ“… Estimated Timeline

**Total duration:** 1-2 days (5-7 hours)

**Breakdown:**
- Phase 9A (MMR): 2-3 hours
  - Implementation: 1 hour
  - Testing (3 experiments): 1.5 hours
  - Analysis: 0.5 hour

- Phase 9B (Reranker): 3-4 hours
  - Model download: 30 min
  - Implementation: 1.5 hours
  - Testing (1-2 experiments): 1.5 hours
  - Analysis: 0.5 hour

- Phase 9C (Combined): 1 hour (optional)

- Documentation: 1 hour

**Can be sequential:**
- If MMR reaches target (0.80+) â†’ Skip Phase 9B
- If MMR fails â†’ Proceed to Phase 9B
- If both fail individually â†’ Try Phase 9C

---

## ğŸ’¡ Key Insights from Phase 8

**What we learned:**
1. âŒ Better embedding (BGE-M3) doesn't always help
2. âŒ Better splitter (Markdown) can make things worse
3. âœ… Simple is often better (MPNet + Recursive + k=3)
4. âœ… 40% of failures are ranking/subsection issues â†’ Reranker promising!
5. âœ… 20% of failures are multi-doc â†’ MMR promising!

**Phase 9 philosophy:**
- Don't change what works (MPNet, Recursive, k optimal)
- Add layers on top (MMR, reranker)
- Focus on known failure patterns
- If it works, it works. If not, accept 0.783 as production.

---

## ğŸš€ Next Actions

**Immediate (Start Phase 9A):**
1. â³ Modify `z3_core/rag.py` to support MMR
2. â³ Create configs for MMR experiments (3 variants)
3. â³ Run MMR experiments
4. â³ Analyze results

**Short-term (Phase 9B if needed):**
5. Install bge-reranker model
6. Implement reranker pipeline
7. Run reranker experiments
8. Compare with baseline

**Medium-term (Finalize):**
9. Create PHASE_9_SUMMARY.md
10. Update PROGRESS.md
11. Decide production config
12. Deploy or move to other advanced techniques

---

**Status:** Ready to start Phase 9A (MMR) â³
**Expected outcome:** Precision 0.80+ (reach target!) or accept 0.783 as production ceiling
**Time investment:** 5-7 hours total (much less than Phase 8B+8C = 9 hours with 0% gain)
